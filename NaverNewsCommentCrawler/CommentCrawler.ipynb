{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./chromedriver_win32/chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlByPeriod(keyword, startDate, endDate):\n",
    "    # 수집된 기사의 링크 목록\n",
    "    newsSiteList = []\n",
    "    url = \"https://search.naver.com/search.naver?&where=news\"\n",
    "    url = url + \"&query=\" + query\n",
    "    url = url + \"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3\"\n",
    "    url = url + \"&ds=\" + startDate + \"&de=\" + endDate\n",
    "    url = url + \"&docid=&nso=so:r,p:from\" + startDate.replace(\".\",\"\") + \"to\"+ endDate.replace(\".\",\"\") + \",a:all&mynews=0&cluster_rank=29&start=1&refresh_start=0\"\n",
    "    \n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #print(soup.prettify())\n",
    "    newsSiteList = soup.find_all('a', '_sp_each_url')\n",
    "    \n",
    "    newsCommentList = []\n",
    "    for newsIndex in range(len(newsSiteList)):\n",
    "        driver.get(newsSiteList[newsIndex]['href']+\"&m_view=1\") # m_view=1 을 붙이면 댓글 창으로 이동\n",
    "        timeout = 5\n",
    "        try:\n",
    "            element_present = EC.presence_of_element_located((By.ID, 'main'))\n",
    "            WebDriverWait(driver, timeout).until(element_present)\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load\")\n",
    "        finally:\n",
    "            print(\"Page loaded\")\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #print(soup.prettify())\n",
    "        newsCommentList = newsCommentList + soup.find_all('span', 'u_cbox_contents')\n",
    "         \n",
    "    return newsCommentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrawlByArticleUrl(url):\n",
    "\n",
    "    driver.get(url+\"&m_view=1\") # m_view=1 을 붙이면 댓글 창으로 이동\n",
    "    timeout = 5\n",
    "    try:\n",
    "        element_present = EC.presence_of_element_located((By.ID, 'main'))\n",
    "        WebDriverWait(driver, timeout).until(element_present)\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "    finally:\n",
    "        print(\"Page loaded\")\n",
    "    \n",
    "    try:\n",
    "        while 1:\n",
    "            #button = driver.find_element_by_css_selector(\".u_cbox_page_more\")\n",
    "            #driver.implicitly_wait(10)\n",
    "            button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.u_cbox_page_more')))\n",
    "            button.click()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    #print(soup.prettify())\n",
    "    newsCommentList = soup.find_all('span', 'u_cbox_contents')\n",
    "         \n",
    "    return newsCommentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 검색 키워드와 기간을 입력\n",
    "keyword = \"여행\"\n",
    "startDate = \"2018.02.01\"\n",
    "endDate = \"2018.02.01\"\n",
    "newsCommentList = CrawlByPeriod(query, start, end)\n",
    "print(len(newsCommentList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newsCommentList = CrawlByArticleUrl(\"https://news.naver.com/main/ranking/read.nhn?mid=etc&sid1=111&rankingType=popular_day&oid=023&aid=0003561349&date=20200912&type=1&rankingSeq=9&rankingSectionId=100\")\n",
    "newsCommentList += CrawlByArticleUrl(\"https://news.naver.com/main/ranking/read.nhn?rankingType=popular_memo&oid=025&aid=0003034634&date=20200913&type=1&rankingSectionId=102&rankingSeq=1\")\n",
    "print(len(newsCommentList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 댓글을 csv 파일로 저장\n",
    "csvFilename = 'NewsComment.csv'\n",
    "csvfile = open(csvFilename, 'a', newline='', encoding='utf-8-sig')\n",
    "csvwriter = csv.writer(csvfile, delimiter='\\t')\n",
    "\n",
    "for cmtIndex in range(len(newsCommentList)):\n",
    "    print(cmtIndex, newsCommentList[cmtIndex].text)\n",
    "    csvwriter.writerow(newsCommentList[cmtIndex].text)\n",
    "    \n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
